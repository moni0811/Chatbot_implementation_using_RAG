# PDF Chatbot using Gemini 1.5 Flash and LangChain

This repository contains a chatbot that can answer questions and summarize content from a PDF document. It uses Google's Gemini 1.5 Flash model for language generation and LangChain for text processing and chunking.

## ğŸ§  Project Overview

The chatbot is designed to:
- Load a PDF file and extract clean text using PyMuPDF (`fitz`)
- Split the text into smaller chunks while retaining page numbers
- Generate embeddings using Gemini's embedding model
- Store and retrieve embeddings using DocArray for fast similarity search
- Answer general questions or summarize specific pages using Gemini
- Maintain chat history for contextual responses

## âœ¨ Features

- ğŸ“„ Load and process unstructured PDF documents
- âœ‚ï¸ Split long text into manageable chunks with page metadata
- ğŸ” Semantic search using Gemini embeddings
- ğŸ§  Context-aware responses with chat history
- ğŸ” Secure API key input using `getpass`
- ğŸ“š Summarize specific pages or answer general questions

## âš™ï¸ Installation

1. Clone the repository:
   ```bash
   git clone https://github.com/moni0811/Chatbot_implementation_using_RAG.git
   cd Chatbot_implementation_using_RAG
   ```
2. Install dependencies:
   ```
   pip install langchain_google_genai langchain_community langchain docarray pymupdf google.generativeai
   ```

## ğŸ§© Dependencies
- langchain_google_genai
- langchain_community
- langchain
- docarray
- PyMuPDF (fitz)
- google.generativeai
 - os, re, getpass

# ğŸš€ Usage

- Run the script:
    - Enter your Gemini API key securely when prompted.
    - Provide the path to your PDF file.
    - Start chatting! Example queries:
      - â€œWhat is the context of this PDF?â€
      - â€œSummarize page 50â€
      - â€œWho are the authors?â€

# ğŸ“Œ Notes
- The chatbot uses Gemini 1.5 Flash for both summarization and question answering.
- Chat history is included in prompts for better contextual understanding.
- Page-specific queries are handled by retaining metadata during chunking.
