{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "collapsed": true,
        "id": "kiM7tgAblqA-",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3fb1a0a5-c924-406f-bbbd-199791b3a23f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/42.0 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m42.0/42.0 kB\u001b[0m \u001b[31m2.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/2.5 MB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m \u001b[32m2.5/2.5 MB\u001b[0m \u001b[31m146.2 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.5/2.5 MB\u001b[0m \u001b[31m64.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m302.8/302.8 kB\u001b[0m \u001b[31m21.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m24.1/24.1 MB\u001b[0m \u001b[31m63.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m438.1/438.1 kB\u001b[0m \u001b[31m29.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m363.0/363.0 kB\u001b[0m \u001b[31m23.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m44.4/44.4 kB\u001b[0m \u001b[31m3.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m50.9/50.9 kB\u001b[0m \u001b[31m4.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ],
      "source": [
        "# Install dependent libraries\n",
        "!pip install --quiet langchain_google_genai langchain_community langchain docarray pymupdf google.generativeai"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 592
        },
        "id": "CGTxlu7XZeGc",
        "outputId": "a12b05f9-9ca1-456e-c5fb-15a631c7edb2"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "RAG Chatbot with Native Google AI API\n",
            "\n",
            "Enter Gemini API Key: ··········\n",
            "Enter path to your PDF file: /content/Book.pdf\n",
            "\n",
            "Starting interactive chat session...\n",
            "   Ask questions about your PDF!\n",
            "Loaded PDF with 218 pages\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/pydantic/_migration.py:283: UserWarning: `pydantic.error_wrappers:ValidationError` has been moved to `pydantic:ValidationError`.\n",
            "  warnings.warn(f'`{import_path}` has been moved to `{new_location}`.')\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Chatbot initialized successfully with /content/Book.pdf\n",
            "\n",
            "Chatbot ready! Type 'quit' to exit.\n",
            "\n",
            "You: what is the context of pdf\n",
            "--------------------------------------------------\n",
            "\n",
            " Chat Bot: Based on the provided text snippets, the context of the PDF appears to be a book about customer experience, focusing on understanding and improving customer requirements and implementing changes within organizations.  The excerpts touch upon topics like organizational orientation (customer-centric vs. other priorities), customer desires (conscious, subconscious, and deceptive), and the challenges of implementing customer experience improvements within companies.\n",
            "\n",
            "You: summarize page 50\n",
            "--------------------------------------------------\n",
            "\n",
            "Summary of Page 50:\n",
            "Page 50 argues that business research often overemphasizes rational, conscious decision-making (like that measured by surveys), neglecting the much larger, subconscious influences on consumer behavior.  It uses the metaphor of an iceberg:  the easily observable rational factors are the tip, while the significant, underlying emotional and subconscious drivers are the much larger submerged portion.  The author implies that a more complete understanding requires looking beyond readily quantifiable data.\n",
            "\n",
            "You: who are the authors\n",
            "--------------------------------------------------\n",
            "\n",
            " Chat Bot: Based on the provided text, the authors are Colin Shaw, Steven Walden, and Qaalfa Dibeehi.\n",
            "\n",
            "You: quit\n",
            "Goodbye!\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "import re\n",
        "import getpass\n",
        "import fitz\n",
        "import google.generativeai as genai\n",
        "from langchain.text_splitter import CharacterTextSplitter\n",
        "from langchain_google_genai import GoogleGenerativeAIEmbeddings\n",
        "from langchain_community.vectorstores import DocArrayInMemorySearch\n",
        "from langchain.docstore.document import Document\n",
        "\n",
        "\n",
        "class ChatBot:\n",
        "    \"\"\"\n",
        "    RAG chatbot using native Google Generative AI API\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, pdf_path, api_key=None):\n",
        "        ## Initialize the chatbot with a PDF file\n",
        "\n",
        "        # Set up API key\n",
        "        if api_key:\n",
        "            os.environ[\"GOOGLE_API_KEY\"] = api_key\n",
        "            genai.configure(api_key=api_key)\n",
        "        else:\n",
        "            genai.configure(api_key=os.getenv(\"GOOGLE_API_KEY\"))\n",
        "\n",
        "        # Load and process PDF\n",
        "        self.vector_db, self.split_docs = self._load_and_process_pdf(pdf_path)\n",
        "\n",
        "        # Initialize native Google AI model\n",
        "        self.model = genai.GenerativeModel('gemini-1.5-flash')\n",
        "\n",
        "        # Chat history\n",
        "        self.chat_history = []\n",
        "\n",
        "        print(f\"Chatbot initialized successfully with {pdf_path}\")\n",
        "\n",
        "    def _load_and_process_pdf(self, pdf_path):\n",
        "        ## Load PDF and create vector database\n",
        "\n",
        "        if not os.path.exists(pdf_path):\n",
        "            raise FileNotFoundError(f\"PDF file not found: {pdf_path}\")\n",
        "\n",
        "        # Load the documents\n",
        "        doc = fitz.open(pdf_path)\n",
        "        documents = []\n",
        "        for i, page in enumerate(doc):\n",
        "          text = page.get_text()\n",
        "          documents.append(Document(page_content=text, metadata={\"page\": i + 1}))\n",
        "        doc.close()\n",
        "\n",
        "        print(f\"Loaded PDF with {len(documents)} pages\")\n",
        "\n",
        "        # Split the documents\n",
        "        text_splitter = CharacterTextSplitter(\n",
        "            chunk_size=1000,\n",
        "            chunk_overlap=100,\n",
        "            separator=\"\\n\"\n",
        "        )\n",
        "\n",
        "        split_docs = []\n",
        "        for doc in documents:\n",
        "          chunks=text_splitter.split_text(doc.page_content)\n",
        "          for chunk in chunks:\n",
        "            split_docs.append(Document(page_content=chunk, metadata=doc.metadata.copy()))\n",
        "\n",
        "        # Create embeddings and vector store\n",
        "        embeddings = GoogleGenerativeAIEmbeddings(model='models/embedding-001')\n",
        "        vector_db = DocArrayInMemorySearch.from_documents(split_docs, embeddings)\n",
        "\n",
        "        return vector_db, split_docs\n",
        "\n",
        "    def chat(self):\n",
        "        ## Start interactive chat session\n",
        "\n",
        "        print(\"\\nChatbot ready! Type 'quit' to exit.\\n\")\n",
        "\n",
        "        while True:\n",
        "            question = input(\"You: \").strip()\n",
        "\n",
        "            match = re.search(r'page\\s+(\\d+)', question.lower())\n",
        "            page_number = int(match.group(1)) if match else None\n",
        "\n",
        "            if page_number:\n",
        "              # Try to summarize that specific page\n",
        "              page_docs = [doc for doc in self.split_docs if doc.metadata.get(\"page\") == page_number]\n",
        "\n",
        "              if not page_docs:\n",
        "                print(f\"No content found for page {page_number}\")\n",
        "                continue\n",
        "\n",
        "              # Combine chunks from that page\n",
        "              page_text = \"\\n\\n\".join([doc.page_content for doc in page_docs])\n",
        "\n",
        "              # Create summarization prompt\n",
        "              prompt = f\"\"\"You are a helpful assistant. Please summarize the content of page {page_number} of the document.\n",
        "                        PAGE {page_number} CONTENT:{page_text}\n",
        "                        SUMMARY:\"\"\"\n",
        "\n",
        "              # Use Gemini to generate summary\n",
        "              response = self.model.generate_content(prompt)\n",
        "              summary = response.text\n",
        "\n",
        "              # Print the summary\n",
        "              print(\"-\" * 50)\n",
        "              print(f\"\\nSummary of Page {page_number}:\\n{summary}\")\n",
        "              self.chat_history.append(f\"Q: {question}\")\n",
        "              self.chat_history.append(f\"A: {summary}\")\n",
        "              continue  # Skip to next user input\n",
        "\n",
        "            if question.lower() in ['quit', 'exit', 'bye', 'q']:\n",
        "                  print(\"Goodbye!\")\n",
        "                  break\n",
        "\n",
        "            if not question:\n",
        "                continue\n",
        "\n",
        "            # Retrieve relevant documents for a question\n",
        "            retriever = self.vector_db.as_retriever(search_kwargs={\"k\": 4})\n",
        "            relevant_docs = retriever.invoke(question)\n",
        "\n",
        "            # Combine relevant document content\n",
        "            context = \"\\n\\n---\\n\\n\".join([f\"(Page {doc.metadata.get('page', 'unknown')}):\\n{doc.page_content}\" for doc in relevant_docs])\n",
        "\n",
        "            # Add chat history if available\n",
        "            history_text = \"\"\n",
        "            if self.chat_history and len(self.chat_history) > 0:\n",
        "              recent_history = self.chat_history[-4:]  # Last 2 Q&A pairs\n",
        "              history_text = \"\\n\\nPrevious conversation:\\n\" + \"\\n\".join(recent_history)\n",
        "\n",
        "            prompt = f\"\"\"You are a helpful AI assistant. Answer the question based on the provided context from the document. If the answer cannot be found in the context, please say so clearly.\n",
        "                      CONTEXT FROM DOCUMENT:\n",
        "                      {context}\n",
        "                      {history_text}\n",
        "                      QUESTION: {question}\n",
        "                      ANSWER:\"\"\"\n",
        "\n",
        "            # Get response using native Google API\n",
        "            response = self.model.generate_content(prompt)\n",
        "            answer = response.text\n",
        "\n",
        "            # Update chat history\n",
        "            self.chat_history.append(f\"Q: {question}\")\n",
        "            self.chat_history.append(f\"A: {answer}\")\n",
        "\n",
        "            result={\n",
        "                'answer': answer,\n",
        "                'source_documents': relevant_docs,\n",
        "                'num_sources': len(relevant_docs)\n",
        "            }\n",
        "            print(\"-\" * 50)\n",
        "            print(f\"\\n Chat Bot: {result['answer']}\")\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    print(\"RAG Chatbot with Native Google AI API\\n\")\n",
        "\n",
        "    GEMINI_API_KEY = getpass.getpass(\"Enter Gemini API Key: \")\n",
        "    os.environ[\"GOOGLE_API_KEY\"] = GEMINI_API_KEY\n",
        "\n",
        "    # Input file manually\n",
        "    pdf_path = input(\"Enter path to your PDF file: \").strip()\n",
        "\n",
        "    print(\"\\nStarting interactive chat session...\")\n",
        "    print(\"   Ask questions about your PDF!\")\n",
        "    chatbot = ChatBot(pdf_path)\n",
        "    chatbot.chat()\n",
        "\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}